{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 第11回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 課題1. Attentionを用いた機械翻訳モデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. データセットの読み込みと単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Chap10と同様Tanaka Corpusの一部を抽出したデータセット (https://github.com/odashi/small_parallel_enja) を使います.\n",
    "\n",
    "以下のコードによってデータをダウンロードします.コメントアウトを外して一度だけ実行して下さい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%bash\n",
    "wget https://raw.githubusercontent.com/odashi/small_parallel_enja/master/train.en -qO train.en\n",
    "wget https://raw.githubusercontent.com/odashi/small_parallel_enja/master/train.ja -qO train.ja\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(file_path, target):\n",
    "    vocab = set()\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    if target:\n",
    "        w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "        w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1) # 文の先頭・終端記号\n",
    "    else:\n",
    "        w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "    return w2i\n",
    "\n",
    "def encode(sentence, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        encoded_sentence.append(w2i[w])\n",
    "    return encoded_sentence\n",
    "\n",
    "def load_data(file_path, vocab=None, w2i=None, target=True):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(file_path, target)\n",
    "    \n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        s = line.strip().split()\n",
    "        if target:\n",
    "            s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 出力系列にのみ'<s>', '</s>'をつける\n",
    "train_X, e_w2i, e_i2w = load_data('train.en', target=False)\n",
    "train_y, j_w2i, j_i2w = load_data('train.ja', target=True)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.1, random_state=random_state)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X_lens = [len(com) for com in train_X]\n",
    "valid_X_lens = [len(com) for com in valid_X]\n",
    "sorted_train_indexes = sorted(range(len(train_X_lens)), key=lambda x: -train_X_lens[x])\n",
    "sorted_valid_indexes = sorted(range(len(valid_X_lens)), key=lambda x: -valid_X_lens[x])\n",
    "\n",
    "train_X = [train_X[ind] for ind in sorted_train_indexes]\n",
    "train_y = [train_y[ind] for ind in sorted_train_indexes]\n",
    "valid_X = [valid_X[ind] for ind in sorted_valid_indexes]\n",
    "valid_y = [valid_y[ind] for ind in sorted_valid_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. 各層クラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Embedding, LSTM, DenseクラスはChap10で用いたものと同じです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale=0.08):\n",
    "        self.V = tf.Variable(rng.randn(vocab_size, emb_dim).astype('float32') * scale, name='V')\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        return tf.nn.embedding_lookup(self.V, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, m, h_0=None, c_0=None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        # input gate\n",
    "        self.W_xi = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xi')\n",
    "        self.W_hi = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hi')\n",
    "        self.b_i  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_i')\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_xf = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.W_hf = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.b_f  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_f')\n",
    "\n",
    "        # output gate\n",
    "        self.W_xo = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xo')\n",
    "        self.W_ho = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_ho')\n",
    "        self.b_o  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_o')\n",
    "\n",
    "        # cell state\n",
    "        self.W_xc = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xc')\n",
    "        self.W_hc = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hc')\n",
    "        self.b_c  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_c')\n",
    "\n",
    "        # initial state\n",
    "        self.h_0 = h_0\n",
    "        self.c_0 = c_0\n",
    "\n",
    "        # mask\n",
    "        self.m = m\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        def fn(tm1, x_and_m):\n",
    "            h_tm1 = tm1[0]\n",
    "            c_tm1 = tm1[1]\n",
    "            x_t = x_and_m[0]\n",
    "            m_t = x_and_m[1]\n",
    "            # input gate\n",
    "            i_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xi) + tf.matmul(h_tm1, self.W_hi) + self.b_i)\n",
    "\n",
    "            # forget gate\n",
    "            f_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xf) + tf.matmul(h_tm1, self.W_hf) + self.b_f)\n",
    "\n",
    "            # output gate\n",
    "            o_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xo) + tf.matmul(h_tm1, self.W_ho) + self.b_o)\n",
    "\n",
    "            # cell state\n",
    "            c_t = f_t * c_tm1 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(h_tm1, self.W_hc) + self.b_c)\n",
    "            c_t = m_t[:, np.newaxis] * c_t + (1. - m_t[:, np.newaxis]) * c_tm1 # Mask\n",
    "\n",
    "            # hidden state\n",
    "            h_t = o_t * tf.nn.tanh(c_t)\n",
    "            h_t = m_t[:, np.newaxis] * h_t + (1. - m_t[:, np.newaxis]) * h_tm1 # Mask\n",
    "\n",
    "            return [h_t, c_t]\n",
    "\n",
    "        _x = tf.transpose(x, perm=[1, 0, 2])\n",
    "        _m = tf.transpose(self.m)\n",
    "\n",
    "        if self.h_0 == None:\n",
    "            self.h_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "        if self.c_0 == None:\n",
    "            self.c_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "\n",
    "        h, c = tf.scan(fn=fn, elems=[_x, _m], initializer=[self.h_0, self.c_0])\n",
    "        return tf.transpose(h, perm=[1, 0, 2]), tf.transpose(c, perm=[1, 0, 2])\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        # input gate\n",
    "        i_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xi) + tf.matmul(self.h_0, self.W_hi) + self.b_i)\n",
    "\n",
    "        # forget gate\n",
    "        f_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xf) + tf.matmul(self.h_0, self.W_hf) + self.b_f)\n",
    "\n",
    "        # output gate\n",
    "        o_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xo) + tf.matmul(self.h_0, self.W_ho) + self.b_o)\n",
    "\n",
    "        # cell state\n",
    "        c_t = f_t * self.c_0 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(self.h_0, self.W_hc) + self.b_c)\n",
    "\n",
    "        # hidden state\n",
    "        h_t = o_t * tf.nn.tanh(c_t)\n",
    "\n",
    "        return [h_t, c_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        # Xavier\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                        low=-np.sqrt(6/(in_dim + out_dim)),\n",
    "                        high=np.sqrt(6/(in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32'), name='W')\n",
    "        self.b = tf.Variable(tf.zeros([out_dim], dtype=tf.float32), name='b')\n",
    "        self.function = function\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return self.function(tf.einsum('ijk,kl->ijl', x, self.W) + self.b)\n",
    "\n",
    "    def f_prop_test(self, x_t):\n",
    "        return self.function(tf.matmul(x_t, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1. Attention層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "今回は Luong et al., 2015のGlobal attentionモデルを実装します.\n",
    "\n",
    "- \"Effective Approaches to Attention-based Neural Machine Translation\", Minh-Thang Luong et al., EMNLP 2015 https://arxiv.org/abs/1508.04025\n",
    "\n",
    "前回のchap10で実装したモデルは左図, 今回実装するモデルは右図になります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"./attention-1.png\" width=\"1000mm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Decoderの各ステップにおける計算の手順\n",
    "\n",
    "Encoderの各ステップの隠れ層を\n",
    "\n",
    "$$\n",
    "    \\bar{h} = \\{\\bar{h}_1, \\bar{h}_2, \\ldots, \\bar{h}_s, \\ldots, \\bar{h}_S\\}\n",
    "$$\n",
    "\n",
    "Decoderの各ステップの隠れ層を\n",
    "\n",
    "$$\n",
    "    h = \\{h_1, h_2, \\ldots, h_t, \\ldots, h_T\\}\n",
    "$$\n",
    "\n",
    "とします."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Attention Layerの計算手順は以下のようになります.\n",
    "\n",
    "1. まずRNN (or LSTM, GRU, etc.) により, 隠れ層ベクトルを計算します.\n",
    "$$\n",
    "    h_t = \\mathrm{RNN}(h_{t-1}, x_t)\n",
    "$$\n",
    "2. 次に, 入力系列のどのステップに注目するのかの**重み**$a_t(s)$を, score関数 (後述) により計算します.\n",
    "$$\n",
    "    a_t(s) = \\frac{\\exp(\\mathrm{score}(\\bar{h}_s, h_t))}{\\sum^S_{s'=1}\\exp(\\mathrm{score}(\\bar{h}_s, h_t))}\n",
    "$$\n",
    "3. 2.で計算した重みをもとに, Encoderの各ステップの隠れ層に対する重み付き平均ベクトル (**文脈ベクトル**) $c_t$ を計算します.\n",
    "$$\n",
    "    c_t = \\sum^S_{s=1} a_t(s) \\bar{h}_s\n",
    "$$\n",
    "4. 3.で計算した文脈ベクトルと1.で計算した隠れ層ベクトルから, 新しい出力ベクトルを計算します.\n",
    "$$\n",
    "    \\tilde{h}_t = \\tanh(W_h h_t + W_c c_t + b)\n",
    "$$\n",
    "5. 新しい出力ベクトル$\\tilde{h}_t$をもとに各単語の出力確率を計算します.\n",
    "$$\n",
    "    y_t = \\mathrm{softmax}(W_{out}\\tilde{h}_t + b_{out})\n",
    "$$\n",
    "\n",
    "論文では$\\tilde{h}_t$が次のタイムステップのLSTMにfeedされる方法も示されています. 他の論文でもそのようにしているものも多いです. その場合, Attention層とLSTM層は分けずに, 同じクラスで書くことになります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### score関数について\n",
    "\n",
    "Encoderのどこに注目するかを決める関数です. 今回の実装では,\n",
    "\n",
    "$$\n",
    "   \\mathrm{score}(\\bar{h}_s, h_t) = h_t^{\\mathrm{T}} W_a \\bar{h}_s\n",
    "$$\n",
    "\n",
    "としています. これ以外にも, たとえば以下の様なものが提案されています.\n",
    "\n",
    "$$\n",
    "\\mathrm{score}(\\bar{h}_s, h_t) =\n",
    "\\begin{cases}\n",
    "    {h_t}^{\\mathrm{T}} \\bar{h}_s \\\\\n",
    "    v^{\\mathrm{T}} \\tanh(W_{ad} h_t + W_{ae} \\bar{h}_s)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Attentionのマスクについて\n",
    "\n",
    "Chap10と同様, 今回のモデルにおいてもミニバッチ化の際には短い系列に対してpaddingを行います.\n",
    "\n",
    "Encoderのpaddingした部分に対してはattendしたくないので, $\\exp(\\mathrm{score}(\\bar{h}_s, h_t))$ がゼロになるように (score関数の値がとても小さな値になるように) マスクをかけます. 具体的に下の実装では, 実際に単語がある部分に対しては1, paddingの部分には$-10^{10}$をかけるようにしています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self, e_hid_dim, d_hid_dim, out_dim, h_enc, m_h_enc):\n",
    "        self.W_cc = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=[e_hid_dim, out_dim]).astype('float32'), name='W_cc')\n",
    "        self.W_ch = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=[d_hid_dim, out_dim]).astype('float32'), name='W_ch')\n",
    "        self.W_a  = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=[e_hid_dim, d_hid_dim]).astype('float32'), name='W_a')\n",
    "        self.b    = tf.Variable(np.zeros([out_dim]).astype('float32'), name='b')\n",
    "        self.h_enc = h_enc\n",
    "        self.m_h_enc = m_h_enc\n",
    "\n",
    "    def f_prop(self, h_dec):\n",
    "        # self.h_enc: [batch_size(i), enc_length(j), e_hid_dim(k)]\n",
    "        # self.W_a  : [e_hid_dim(k), d_hid_dim(l)]\n",
    "        # -> h_enc: [batch_size(i), enc_length(j), d_hid_dim(l)]\n",
    "        h_enc = tf.einsum('ijk,kl->ijl', self.h_enc, self.W_a)\n",
    "        \n",
    "        # h_dec: [batch_size(i), dec_length(j), d_hid_dim(k)]\n",
    "        # h_enc: [batch_size(i), enc_length(l), d_hid_dim(k)]\n",
    "        # -> score: [batch_size(i), dec_length(j), enc_length(l)]\n",
    "        score = tf.einsum('ijk,ilk->ijl', h_dec, h_enc) # Attention score\n",
    "        \n",
    "        # score  : [batch_size, dec_length, enc_length]\n",
    "        # m_h_enc: [batch_size, enc_length] -> [batch_size, np.newaxis, enc_length]\n",
    "        score = score * self.m_h_enc[:, np.newaxis, :] # Mask\n",
    "        \n",
    "        # encoderのステップにそって正規化する\n",
    "        self.a = tf.nn.softmax(score) # Attention weight\n",
    "        \n",
    "        # self.a  : [batch_size(i), dec_length(j), enc_length(k)]\n",
    "        # self.enc: [batch_size(i), enc_length(k), e_hid_dim(l)]\n",
    "        # -> c: [batch_size(i), dec_length(j), e_hid_dim(l)]\n",
    "        c = tf.einsum('ijk,ikl->ijl', self.a, self.h_enc) # Context vector\n",
    "        \n",
    "        return tf.nn.tanh(tf.einsum('ijk,kl->ijl', c, self.W_cc) + tf.einsum('ijk,kl->ijl', h_dec, self.W_ch) + self.b)\n",
    "    \n",
    "    def f_prop_test(self, h_dec_t):\n",
    "        # self.h_enc: [batch_size(i), enc_length(j), e_hid_dim(k)]\n",
    "        # self.W_a  : [e_hid_dim(k), d_hid_dim(l)]\n",
    "        # -> h_enc: [batch_size(i), enc_length(j), d_hid_dim(l)]\n",
    "        h_enc = tf.einsum('ijk,kl->ijl', self.h_enc, self.W_a)\n",
    "        \n",
    "        # h_dec_t: [batch_size(i), d_hid_dim(j)]\n",
    "        # h_enc  : [batch_size(i), enc_length(k), d_hid_dim(j)]\n",
    "        # -> score: [batch_size(i), enc_length(k)]\n",
    "        score = tf.einsum('ij,ikj->ik', h_dec_t, h_enc) # Attention score\n",
    "        \n",
    "        # score       : [batch_size(i), enc_length(k)]\n",
    "        # self.m_h_enc: [batch_size(i), enc_length(k)]\n",
    "        score = score * self.m_h_enc # Mask\n",
    "        \n",
    "        self.a = tf.nn.softmax(score) # Attention weight\n",
    "        \n",
    "        # self.a    : [batch_size(i), enc_length(j)]\n",
    "        # self.h_enc: [batch_size(i), enc_length(j), e_hid_dim(k)]\n",
    "        # -> c: [batch_size(i), e_hid_dim(k)]\n",
    "        c = tf.einsum('ij,ijk->ik', self.a, self.h_enc) # Context vector\n",
    "\n",
    "        return tf.nn.tanh(tf.matmul(c, self.W_cc) + tf.matmul(h_dec_t, self.W_ch) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. 計算グラフ構築 & パラメータの更新設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e_vocab_size = len(e_w2i)\n",
    "j_vocab_size = len(j_w2i)\n",
    "emb_dim = 128\n",
    "e_hid_dim = 128\n",
    "d_hid_dim = 128\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "m = tf.cast(tf.not_equal(x, -1), tf.float32)\n",
    "d = tf.placeholder(tf.int32, [None, None], name='d')\n",
    "d_in = d[:, :-1]\n",
    "d_out = d[:, 1:]\n",
    "d_out_one_hot = tf.one_hot(d_out, depth=j_vocab_size, dtype=tf.float32)\n",
    "\n",
    "# Encoderの各ステップにたいするattentionのmask\n",
    "# tf.where: https://www.tensorflow.org/api_docs/python/tf/where\n",
    "# 引数\n",
    "# - condition: bool値のtensor\n",
    "# - x: bool値がTrueであるときに入力される値\n",
    "# - y: bool値がFalseであるときに入力される値\n",
    "m_h_enc = tf.where(\n",
    "    condition=tf.equal(x, -1),\n",
    "    x=tf.ones_like(x, dtype=tf.float32)*np.float32(-1e+10), # padding(-1)の部分には-10^10\n",
    "    y=tf.ones_like(x, dtype=tf.float32) # 実際の単語の部分には1\n",
    ")\n",
    "\n",
    "def f_props(layers, x):\n",
    "    for layer in layers:\n",
    "        x = layer.f_prop(x)\n",
    "    return x\n",
    "\n",
    "encoder = [\n",
    "    Embedding(e_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, e_hid_dim, m)\n",
    "]\n",
    "\n",
    "h_enc, c_enc = f_props(encoder, x)\n",
    "\n",
    "decoder_pre = [\n",
    "    Embedding(j_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, d_hid_dim, m=tf.ones_like(d_in, dtype='float32'), h_0=h_enc[:, -1, :], c_0=c_enc[:, -1, :])\n",
    "]\n",
    "\n",
    "decoder_post = [\n",
    "    Attention(e_hid_dim, d_hid_dim, d_hid_dim, h_enc, m_h_enc),\n",
    "    Dense(d_hid_dim, j_vocab_size, tf.nn.softmax)\n",
    "]\n",
    "\n",
    "h_dec, c_dec = f_props(decoder_pre, d_in)\n",
    "y = f_props(decoder_post, h_dec)\n",
    "\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(d_out_one_hot * tf.log(tf.clip_by_value(y, 1e-10, 1.0)), axis=[1, 2]))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 128\n",
    "n_batches_train = len(train_X)//batch_size\n",
    "n_batches_valid = len(valid_X)//batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    train_costs = []\n",
    "    for i in range(n_batches_train):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train_X_mb = np.array(pad_sequences(train_X[start:end], padding='post', value=-1))\n",
    "        train_y_mb = np.array(pad_sequences(train_y[start:end], padding='post', value=-1))\n",
    "        \n",
    "        _, train_cost = sess.run([train, cost], feed_dict={x: train_X_mb, d: train_y_mb})\n",
    "        train_costs.append(train_cost)\n",
    "\n",
    "    # Valid\n",
    "    valid_costs = []\n",
    "    for i in range(n_batches_valid):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        valid_X_mb = np.array(pad_sequences(valid_X[start:end], padding='post', value=-1))\n",
    "        valid_y_mb = np.array(pad_sequences(valid_y[start:end], padding='post', value=-1))\n",
    "        \n",
    "        valid_cost = sess.run(cost, feed_dict={x: valid_X_mb, d: valid_y_mb})\n",
    "        valid_costs.append(valid_cost)\n",
    "\n",
    "    print('EPOCH: %i, Training cost: %.3f, Validation cost: %.3f' % (epoch+1, np.mean(train_costs), np.mean(valid_costs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. 翻訳文の生成とattention weightの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.1. グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_0 = tf.constant(0)\n",
    "h_0 = tf.placeholder(tf.float32, [None, None], name='h_0')\n",
    "c_0 = tf.placeholder(tf.float32, [None, None], name='c_0')\n",
    "y_0 = tf.placeholder(tf.int32, [None, None], name='y_0')\n",
    "a_0 = tf.zeros_like(decoder_post[0].h_enc[:, :, :1], dtype=tf.float32, name='a_0')\n",
    "f_0 = tf.cast(tf.zeros_like(y_0[:, 0]), dtype=tf.bool) # バッチ内の各サンプルに対して</s>が出たかどうかのflag\n",
    "\n",
    "f_0_size = tf.reduce_sum(tf.ones_like(f_0, dtype=tf.int32))\n",
    "max_len = tf.placeholder(tf.int32, name='max_len') # iterationの繰り返し回数の限度\n",
    "\n",
    "def f_props_test(layers, x_t):\n",
    "    for layer in layers:\n",
    "        x_t = layer.f_prop_test(x_t)\n",
    "    return x_t\n",
    "\n",
    "def cond(t, h_t, c_t, y_t, a_t, f_t):\n",
    "    num_true = tf.reduce_sum(tf.cast(f_t, tf.int32)) # Trueの数\n",
    "    unfinished = tf.not_equal(num_true, f_0_size)\n",
    "    return tf.logical_and(t+1 < max_len, unfinished)\n",
    "\n",
    "def body(t, h_tm1, c_tm1, y, a, f_tm1):\n",
    "    y_tm1 = y[:, -1]\n",
    "\n",
    "    decoder_pre[1].h_0 = h_tm1\n",
    "    decoder_pre[1].c_0 = c_tm1\n",
    "    h_t, c_t = f_props_test(decoder_pre, y_tm1)\n",
    "    y_t = tf.cast(tf.argmax(f_props_test(decoder_post, h_t), axis=1), tf.int32)\n",
    "    a_t = decoder_post[0].a\n",
    "    \n",
    "    y = tf.concat([y, y_t[:, np.newaxis]], axis=1)\n",
    "    a = tf.concat([a, a_t[:, :, np.newaxis]], axis=2)\n",
    "    \n",
    "    f_t = tf.logical_or(f_tm1, tf.equal(y_t, 1))\n",
    "    \n",
    "    return [t+1, h_t, c_t, y, a, f_t]\n",
    "\n",
    "res = tf.while_loop(\n",
    "    cond,\n",
    "    body,\n",
    "    loop_vars=[t_0, h_0, c_0, y_0, a_0, f_0],\n",
    "    shape_invariants=[\n",
    "        t_0.get_shape(),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None, None]),\n",
    "        tf.TensorShape([None])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.2. 初期値$\\bar{h}, h_0, c_0, y_0$の獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_X_mb = pad_sequences(valid_X[:100], padding='post', value=-1)\n",
    "_y_0 = np.zeros_like(valid_X[:100], dtype='int32')[:, np.newaxis]\n",
    "_m_h_enc, _h_enc, _c_enc = sess.run([m_h_enc, h_enc, c_enc], feed_dict={x: valid_X_mb})\n",
    "_h_0 = _h_enc[:, -1, :]\n",
    "_c_0 = _c_enc[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.3. 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _, _, pred_y, att_weights, _ = sess.run(res, feed_dict={\n",
    "    decoder_post[0].m_h_enc: _m_h_enc,\n",
    "    decoder_post[0].h_enc: _h_enc,\n",
    "    y_0: _y_0,\n",
    "    h_0: _h_0,\n",
    "    c_0: _c_0,\n",
    "    max_len: 100\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.4. 生成例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num = 5\n",
    "\n",
    "origx = valid_X[num]\n",
    "predy = pred_y[num].tolist()[1:pred_y[num].tolist().index(1)]\n",
    "truey = valid_y[num][1:-1]\n",
    "\n",
    "print('元の文:', ' '.join([e_i2w[com] for com in origx]))\n",
    "print('生成文された文:', ' '.join([j_i2w[com] for com in predy]))\n",
    "print('正解文:', ' '.join([j_i2w[com] for com in truey]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.5. アテンションの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "matplotlibで日本語を出力するために日本語フォントをインストールします."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ~/.cache/matplotlib/fontList.py3k.cache\n",
    "wget -q https://s3.amazonaws.com/ilect-public/ail/TakaoPGothic.ttf # Takaoフォント (https://launchpad.net/takao-fonts)\n",
    "mv TakaoPGothic.ttf /usr/local/share/fonts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "アテンションで獲得した重み$a$を可視化してみます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = att_weights[num][:len(origx), 1:len(predy)+1]\n",
    "\n",
    "xticks = [j_i2w[com] for com in predy]\n",
    "yticks = [e_i2w[com] for com in origx]\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "ax.imshow(a, interpolation='nearest', cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "for (_y, _x), weight in np.ndenumerate(a):\n",
    "    ax.text(_x, _y, '{:.2f}'.format(weight), ha='center', va='center', color=str(weight), fontweight='bold')\n",
    "ax.set_yticks(np.arange(a.shape[0]))\n",
    "ax.set_xticks(np.arange(a.shape[1]))\n",
    "ax.set_yticklabels(yticks, fontweight='bold', fontsize='12', color='blue')\n",
    "ax.set_xticklabels(xticks, fontweight='bold', fontsize='12', color='red', fontdict={'family': 'TakaoPGothic'})\n",
    "ax.xaxis.tick_top()\n",
    "ax.yaxis.set_ticks_position('none')\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
