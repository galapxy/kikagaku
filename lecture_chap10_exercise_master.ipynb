{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 第10回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 課題1. Recurrent Neural Network (RNN) Encoder-Decoderモデルで英日翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. データセットの読み込みと単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.1. データセットについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "train.enとtrain.jaの中身は次のようになっています.\n",
    "\n",
    "train.enの中身 (英語の文)\n",
    "```\n",
    "i can 't tell who will arrive first .\n",
    "many animals have been destroyed by men .\n",
    "i 'm in the tennis club .\n",
    "︙\n",
    "```\n",
    "\n",
    "train.jaの中身(日本語の文, 対訳)\n",
    "```\n",
    "誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n",
    "多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n",
    "私 は テニス 部員 で す 。\n",
    "︙\n",
    "```\n",
    "(データセットにはTanaka Corpus ( http://www.edrdg.org/wiki/index.php/Tanaka_Corpus )の一部を抽出した \n",
    "small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods ( https://github.com/odashi/small_parallel_enja ) を使っています.)\n",
    "\n",
    "以下のコードによってデータをダウンロードします.コメントアウトを外して一度だけ実行して下さい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%bash\n",
    "wget https://raw.githubusercontent.com/odashi/small_parallel_enja/master/train.en -qO train.en\n",
    "wget https://raw.githubusercontent.com/odashi/small_parallel_enja/master/train.ja -qO train.ja\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2. 単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "単語のままだと扱いづらいので, それぞれの単語をIDに置き換えます. 以下のコードでは, まず`build_vocab`で単語->idの辞書(`w2i`)を作り, それを元に`encode`で各単語をid化しています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(file_path):\n",
    "    vocab = set()\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "    w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1) # 文の先頭・終端記号\n",
    "\n",
    "    return w2i\n",
    "\n",
    "def encode(sentence, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        encoded_sentence.append(w2i[w])\n",
    "    return encoded_sentence\n",
    "\n",
    "def load_data(file_path, vocab=None, w2i=None):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(file_path)\n",
    "    \n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        s = line.strip().split()\n",
    "        s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 英語->日本語\n",
    "train_X, e_w2i, e_i2w = load_data('train.en')\n",
    "train_y, j_w2i, j_i2w = load_data('train.ja')\n",
    "\n",
    "train_X, _, train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42) # 演習用に縮小\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.02, random_state=42)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. 各層クラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "以下のクラスの中では, 系列中の全てのステップに対してまとめて処理をおこなう`f_prop`関数の他に, 1つのステップに対してのみ処理をおこなう`f_prop_test`関数を実装しています. 理由は後述します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1. 単語のEmbedding層\n",
    "\n",
    "$m$ : emb_dim\n",
    "\n",
    "$n$ : vocab_size\n",
    "\n",
    "実際にEmbedding層の処理を担うembedding_lookupでは, 入力をone_hotベクトルに変換し, Embedding層の行列に掛け, 対応する列ベクトルを選択します.\n",
    "![embedding](embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale=0.08):\n",
    "        self.V = tf.Variable(rng.randn(vocab_size, emb_dim).astype('float32') * scale, name='V')\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        return tf.nn.embedding_lookup(self.V, x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2. Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "実装する式は次のようになります. ($\\odot$は要素ごとの積)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 入力ゲート: $\\hspace{20mm}i_t = \\sigma \\left( W_{xi} x_t + W_{hi} h_{t-1} + b_i \\right)$\n",
    "- 忘却ゲート: $\\hspace{20mm}f_t = \\sigma \\left( W_{xf} x_t + W_{hf} h_{t-1} + b_f \\right)$  \n",
    "- 出力ゲート: $\\hspace{20mm}o_t = \\sigma \\left( W_{xo} x_t + W_{ho} h_{t-1} + b_o \\right)$  \n",
    "- セル:　　　 $\\hspace{20mm}c_t = f_t \\odot c_{t-1} + i_t \\odot \\tanh \\left( W_{xc} x_t + W_{hc} h_{t-1} + b_c \\right)$  \n",
    "- 隠れ層: 　　$\\hspace{20mm}h_t = o_t \\odot \\tanh \\left( c_t \\right)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "単純なRNNでは各ステップの関数の戻り値は隠れ層のみ ($h_t$) でしたが, LSTMでは隠れ層とセル状態の2つ ($h_t, c_t$) となるので注意してください. またマスクに関しても両方に適用する必要があります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, m, h_0=None, c_0=None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        # input gate\n",
    "        self.W_xi = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xi')\n",
    "        self.W_hi = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hi')\n",
    "        self.b_i  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_i')\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_xf = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.W_hf = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.b_f  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_f')\n",
    "\n",
    "        # output gate\n",
    "        self.W_xo = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xo')\n",
    "        self.W_ho = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_ho')\n",
    "        self.b_o  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_o')\n",
    "\n",
    "        # cell state\n",
    "        self.W_xc = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xc')\n",
    "        self.W_hc = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hc')\n",
    "        self.b_c  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_c')\n",
    "\n",
    "        # initial state\n",
    "        self.h_0 = h_0\n",
    "        self.c_0 = c_0\n",
    "\n",
    "        # mask\n",
    "        self.m = m\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        def fn(tm1, x_and_m):\n",
    "            h_tm1 = tm1[0]\n",
    "            c_tm1 = tm1[1]\n",
    "            x_t = x_and_m[0]\n",
    "            m_t = x_and_m[1]\n",
    "            # input gate\n",
    "            i_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xi) + tf.matmul(h_tm1, self.W_hi) + self.b_i)\n",
    "\n",
    "            # forget gate\n",
    "            f_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xf) + tf.matmul(h_tm1, self.W_hf) + self.b_f)\n",
    "\n",
    "            # output gate\n",
    "            o_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xo) + tf.matmul(h_tm1, self.W_ho) + self.b_o)\n",
    "\n",
    "            # cell state\n",
    "            c_t = f_t * c_tm1 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(h_tm1, self.W_hc) + self.b_c)\n",
    "            c_t = m_t[:, np.newaxis] * c_t + (1. - m_t[:, np.newaxis]) * c_tm1 # Mask\n",
    "\n",
    "            # hidden state\n",
    "            h_t = o_t * tf.nn.tanh(c_t)\n",
    "            h_t = m_t[:, np.newaxis] * h_t + (1. - m_t[:, np.newaxis]) * h_tm1 # Mask\n",
    "\n",
    "            return [h_t, c_t]\n",
    "\n",
    "        _x = tf.transpose(x, perm=[1, 0, 2])\n",
    "        _m = tf.transpose(self.m)\n",
    "\n",
    "        if self.h_0 == None:\n",
    "            self.h_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "        if self.c_0 == None:\n",
    "            self.c_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "\n",
    "        h, c = tf.scan(fn=fn, elems=[_x, _m], initializer=[self.h_0, self.c_0])\n",
    "        return tf.transpose(h, perm=[1, 0, 2]), tf.transpose(c, perm=[1, 0, 2])\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        # input gate\n",
    "        i_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xi) + tf.matmul(self.h_0, self.W_hi) + self.b_i)\n",
    "\n",
    "        # forget gate\n",
    "        f_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xf) + tf.matmul(self.h_0, self.W_hf) + self.b_f)\n",
    "\n",
    "        # output gate\n",
    "        o_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xo) + tf.matmul(self.h_0, self.W_ho) + self.b_o)\n",
    "\n",
    "        # cell state\n",
    "        c_t = f_t * self.c_0 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(self.h_0, self.W_hc) + self.b_c)\n",
    "\n",
    "        # hidden state\n",
    "        h_t = o_t * tf.nn.tanh(c_t)\n",
    "\n",
    "        return [h_t, c_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.3. 全結合層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`f_prop`における入力は3階テンソルとなるので, `tf.einsum`を使用します.\n",
    "\n",
    "入力`x`と重み`W`のshapeは, それぞれ\n",
    "\n",
    "- `x`: (ミニバッチサイズ, i) x (系列長, j) x (入力次元数, k)\n",
    "- `W`: (入力次元数, k) x (出力次元数, l)\n",
    "\n",
    "で, 出力は\n",
    "\n",
    "- (ミニバッチサイズ, i) x (系列長, j) x (出力次元数, l)\n",
    "\n",
    "となるので, `einsum`の第一引数の表記は`'ijk,kl->ijl'`となります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        # Xavier\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                        low=-np.sqrt(6/(in_dim + out_dim)),\n",
    "                        high=np.sqrt(6/(in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32'), name='W')\n",
    "        self.b = tf.Variable(tf.zeros([out_dim], dtype=tf.float32), name='b')\n",
    "        self.function = function\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return self.function(tf.einsum('ijk,kl->ijl', x, self.W) + self.b)\n",
    "\n",
    "    def f_prop_test(self, x_t):\n",
    "        return self.function(tf.matmul(x_t, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. 計算グラフ構築 & パラメータの更新設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "下の図のモデルを実装します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![seq2seq](seq2seq.png)\n",
    "\n",
    "https://www.tensorflow.org/tutorials/seq2seq より引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ミニバッチサイズ, 系列長, 辞書のサイズをそれぞれ$N$, $T$, $K$とすると, 多クラス交差エントロピー誤差関数は次のようになります.\n",
    "\n",
    "$$\n",
    "    E({\\bf \\theta}) = -\\frac{1}{N}\\sum^N_{n=1}\\sum^T_{t=1}\\sum^K_{k=1} d^{(n)}_{t, k} \\log y^{(n)}_{t, k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Encoder, Decoderともに短い系列に対してはpaddingをします. また, Encoderではマスクを行います.\n",
    "\n",
    "Decoderでpaddingした部分については, コストがゼロになるようにします. これは, 単語がある部分を1, paddingの部分を0とするバイナリのマスクをかけるか, paddingの部分の教師ラベルdの要素をすべてゼロになるようにします.\n",
    "\n",
    "`tf`においては, `tf.one_hot`でone_hot化するときに範囲外の値(-1など)を入力とすれば, その値に対するベクトルはすべてゼロとなります. 以下ではこちらの方法で実装しています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "e_vocab_size = len(e_w2i)\n",
    "j_vocab_size = len(j_w2i)\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "m = tf.cast(tf.not_equal(x, -1), tf.float32)\n",
    "d = tf.placeholder(tf.int32, [None, None], name='d')\n",
    "d_in = d[:, :-1]\n",
    "\n",
    "d_out = d[:, 1:]\n",
    "d_out_one_hot = tf.one_hot(d_out, depth=j_vocab_size, dtype=tf.float32)\n",
    "\n",
    "def f_props(layers, x):\n",
    "    for layer in layers:\n",
    "        x = layer.f_prop(x)\n",
    "    return x\n",
    "\n",
    "encoder = [\n",
    "    Embedding(e_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, hid_dim, m)\n",
    "]\n",
    "\n",
    "h_enc, c_enc = f_props(encoder, x)\n",
    "\n",
    "decoder_pre = [\n",
    "    Embedding(j_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, hid_dim, tf.ones_like(d_in, dtype='float32'), h_0=h_enc[:, -1, :], c_0=c_enc[:, -1, :]),\n",
    "]\n",
    "\n",
    "decoder_post = [\n",
    "    Dense(hid_dim, j_vocab_size, tf.nn.softmax)\n",
    "]\n",
    "\n",
    "h_dec, c_dec = f_props(decoder_pre, d_in)\n",
    "y = f_props(decoder_post, h_dec)\n",
    "\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(d_out_one_hot * tf.log(tf.clip_by_value(y, 1e-10, 1.0)), axis=[1, 2]))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X_lens = [len(com) for com in train_X]\n",
    "sorted_train_indexes = sorted(range(len(train_X_lens)), key=lambda x: -train_X_lens[x])\n",
    "\n",
    "train_X = [train_X[ind] for ind in sorted_train_indexes]\n",
    "train_y = [train_y[ind] for ind in sorted_train_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "n_batches = len(train_X) // batch_size\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    train_costs = []\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        train_X_mb = np.array(pad_sequences(train_X[start:end], padding='post', value=-1))\n",
    "        train_y_mb = np.array(pad_sequences(train_y[start:end], padding='post', value=-1))\n",
    "\n",
    "        _, train_cost = sess.run([train, cost], feed_dict={x: train_X_mb, d: train_y_mb})\n",
    "        train_costs.append(train_cost)\n",
    "\n",
    "    # valid\n",
    "    valid_X_mb = np.array(pad_sequences(valid_X, padding='post', value=-1))\n",
    "    valid_y_mb = np.array(pad_sequences(valid_y, padding='post', value=-1))\n",
    "\n",
    "    valid_cost = sess.run(cost, feed_dict={x: valid_X_mb, d: valid_y_mb})\n",
    "    print('EPOCH: %i, Training cost: %.3f, Validation cost: %.3f' % (epoch+1, np.mean(train_costs), valid_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "翻訳文の生成には`while`ループを使うので, まず`tf`におけるwhileループの実装である`tf.while_loop`について説明します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  5.1. `tf.while_loop`関数  \\[[link\\]](https://www.tensorflow.org/api_docs/python/tf/while_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "主な引数は以下のとおりです.\n",
    "\n",
    "- 第1引数 `cond`: `True` or `False` を返す関数 (正確にはcallable)\n",
    "- 第2引数 `body`: 各iterationで実行する関数 (正確にはcallable)\n",
    "- 第3引数 `loop_vars`: `cond`及び`body`に最初に渡される変数\n",
    "\n",
    "`cond`で指定された関数の戻り値が`True`である限り`body`で指定された関数を実行し続けます. そして, `loop_vars`で指定された全ての変数に対して, 最後のiteration後の値を返します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "例えば, 入力が5未満であるかぎり1ずつ足す処理を実行したい場合, コードは次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g0 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "def cond(z):\n",
    "    return z < 5\n",
    "\n",
    "def body(z):\n",
    "    return z + 1\n",
    "\n",
    "with g0.as_default():\n",
    "    z = tf.constant(0)\n",
    "\n",
    "    res = tf.while_loop(cond, body, [z])\n",
    "\n",
    "with tf.Session(graph=g0) as sess_g0:\n",
    "    print(sess_g0.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "また, `tf.while_loop`の各iteration後の変数の`shape`はデフォルトでは同じであることが指定されています. なので, 各iteration後の`shape`が変化する場合は, この条件を緩和する必要があります.\n",
    "\n",
    "たとえば, 上の1ずつ足すプログラムですべてのiteration後の値を保持して返したい場合, 各iterationの戻り値は\n",
    "\n",
    "```\n",
    "[1], [1, 2], [1, 2, 3], [1, 2, 3, 4], ...\n",
    "```\n",
    "となっていくので, それぞれの`shape`は,\n",
    "```\n",
    "(1,), (2,), (3,), (4,), ...\n",
    "```\n",
    "と変化していきます. つまり, この例ではベクトルの次元数が変化していくので, `shape_invariants`で`shape`を`[None]` (実際は`tf.TensorShape([None])`と指定します.\n",
    "\n",
    "具体的なコードは次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g1 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "def cond(z):\n",
    "    return z[-1] < 5\n",
    "\n",
    "def body(z):\n",
    "    return tf.concat([z, z[-1:]+1], axis=0)\n",
    "\n",
    "with g1.as_default():\n",
    "    z = tf.zeros(1)\n",
    "\n",
    "    res = tf.while_loop(\n",
    "        cond,\n",
    "        body,\n",
    "        [z],\n",
    "        shape_invariants=[tf.TensorShape([None])]\n",
    "    )\n",
    "\n",
    "with tf.Session(graph=g1) as sess_g1:\n",
    "    print(sess_g1.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2x1行列に対して同じような操作をしたい場合は次のようになります. この場合各iteration後の行列の`shape`は\n",
    "```\n",
    "(2, 1), (2, 2), (2, 3), (2, 4), ...\n",
    "```\n",
    "と列数のみ変化していくので, `shape_invariants`には`tf.TensorShape([2, None])`と指定します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g2 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "\n",
    "def cond(z):\n",
    "    return tf.reduce_sum(z[:, -1]) < 5*2\n",
    "\n",
    "def body(z):\n",
    "    return tf.concat([z, z[:, -1:]+1], axis=1)\n",
    "\n",
    "with g2.as_default():\n",
    "    z = tf.zeros([2, 1])\n",
    "\n",
    "    res = tf.while_loop(\n",
    "        cond,\n",
    "        body,\n",
    "        [z],\n",
    "        shape_invariants=[tf.TensorShape([2, None])]\n",
    "    )\n",
    "\n",
    "with tf.Session(graph=g2) as sess_g2:\n",
    "    print(sess_g2.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "詳細は公式のドキュメントを参照してください.\n",
    "\n",
    "- tf.while_loop: https://www.tensorflow.org/api_docs/python/tf/while_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.2. グラフの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "未知のデータに対してEncoder-Decoderモデルを適用するとき, 正解ラベル$d$はわからないので, 代わりに前のステップで予測した単語を各ステップでの入力とします. そして, 系列の終わりを表す単語 (`</s>`) が出力されるまで繰り返します.\n",
    "\n",
    "具体的には, $h_{t-1}, c_{t-1}, y_{t-1}$を入力として$y_t$を受け取る操作を, バッチ内の全てのサンプルにおける$y_t$が`</s>`となるまで続けます.\n",
    "\n",
    "毎iterationで1つのステップについてのみ順伝播を計算すれば良いので, ここで各クラスの`f_prop_test`関数を使用します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_0 = tf.constant(0)\n",
    "y_0 = tf.placeholder(tf.int32, [None, None], name='y_0')\n",
    "h_0 = tf.placeholder(tf.float32, [None, None], name='h_0')\n",
    "c_0 = tf.placeholder(tf.float32, [None, None], name='c_0')\n",
    "f_0 = tf.cast(tf.zeros_like(y_0[:, 0]), dtype=tf.bool) # バッチ内の各サンプルに対して</s>が出たかどうかのflag\n",
    "f_0_size = tf.reduce_sum(tf.ones_like(f_0, dtype=tf.int32))\n",
    "max_len = tf.placeholder(tf.int32, name='max_len') # iterationの繰り返し回数の限度\n",
    "\n",
    "def f_props_test(layers, x_t):\n",
    "    for layer in layers:\n",
    "        x_t = layer.f_prop_test(x_t)\n",
    "    return x_t\n",
    "\n",
    "def cond(t, h_t, c_t, y_t, f_t):\n",
    "    num_true = tf.reduce_sum(tf.cast(f_t, tf.int32)) # Trueの数\n",
    "    unfinished = tf.not_equal(num_true, f_0_size)\n",
    "    return tf.logical_and(t+1 < max_len, unfinished)\n",
    "\n",
    "def body(t, h_tm1, c_tm1, y, f_tm1):\n",
    "    y_tm1 = y[:, -1]\n",
    "\n",
    "    decoder_pre[1].h_0 = h_tm1\n",
    "    decoder_pre[1].c_0 = c_tm1\n",
    "    h_t, c_t = f_props_test(decoder_pre, y_tm1)\n",
    "    y_t = tf.cast(tf.argmax(f_props_test(decoder_post, h_t), axis=1), tf.int32)\n",
    "\n",
    "    y = tf.concat([y, y_t[:, np.newaxis]], axis=1)\n",
    "\n",
    "    f_t = tf.logical_or(f_tm1, tf.equal(y_t, 1)) # flagの更新\n",
    "\n",
    "    return [t+1, h_t, c_t, y, f_t]\n",
    "\n",
    "res = tf.while_loop(\n",
    "    cond,\n",
    "    body,\n",
    "    loop_vars=[t_0, h_0, c_0, y_0, f_0],\n",
    "    shape_invariants=[\n",
    "        t_0.get_shape(),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.3. 初期値$h_0, c_0, y_0$の獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_X_mb = pad_sequences(valid_X, padding='post', value=-1)\n",
    "_y_0 = np.zeros_like(valid_X, dtype='int32')[:, np.newaxis]\n",
    "_h_enc, _c_enc = sess.run([h_enc, c_enc], feed_dict={x: valid_X_mb})\n",
    "_h_0 = _h_enc[:, -1, :]\n",
    "_c_0 = _c_enc[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.3. 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, _, _, pred_y, _ = sess.run(res, feed_dict={\n",
    "    y_0: _y_0,\n",
    "    h_0: _h_0,\n",
    "    c_0: _c_0,\n",
    "    max_len: 100\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.4. 生成例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "\n",
    "origy = valid_X[num][1:-1]\n",
    "predy = list(pred_y[num])\n",
    "truey = valid_y[num][1:-1]\n",
    "\n",
    "print('元の文:', ' '.join([e_i2w[com] for com in origy]))\n",
    "print('生成文された文:', ' '.join([j_i2w[com] for com in predy[1:predy.index(1)]]))\n",
    "print('正解文:', ' '.join([j_i2w[com] for com in truey]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
