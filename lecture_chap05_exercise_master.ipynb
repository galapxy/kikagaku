{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. Tensorflowの基礎を学ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tensorflowの概観"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf`では, 基本的に以下の流れで機械学習モデルを構築します.\n",
    "\n",
    "1. プレースホルダーと変数の設定\n",
    "2. グラフの構築\n",
    "3. 誤差関数の設定\n",
    "4. 重みの更新ルールの設定\n",
    "5. `tf.Session()`を開始して学習\n",
    "6. 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 線形回帰の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフのリセット\n",
    "\n",
    "# Step1. プレースホルダー・変数の設定\n",
    "## placeholder: データを流し込む変数. データ毎に変わる\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "t = tf.placeholder(tf.float32, name='t')\n",
    "\n",
    "## Variable: 変数(重み). データ間で共有される\n",
    "w = tf.Variable(0.0, name='w')\n",
    "b = tf.Variable(0.0, name='b')\n",
    "\n",
    "# Step2. グラフの構築\n",
    "y = w*x + b\n",
    "\n",
    "# Step3. 誤差関数の設定\n",
    "cost = tf.reduce_mean((y - t)**2)\n",
    "\n",
    "# Step4. 重みの更新則の設定\n",
    "gw, gb = tf.gradients(cost, [w, b]) # 勾配の計算\n",
    "updates = [\n",
    "    w.assign(w - 0.1*gw), # 勾配降下法\n",
    "    b.assign(b - 0.1*gb)\n",
    "]\n",
    "train = tf.group(*updates)\n",
    "\n",
    "# Step.5. 学習 (y = 2*x + 3)\n",
    "data_X = np.array([0., 1., 2., 3., 4.])\n",
    "data_y = np.array([3., 5., 7., 9., 11.])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) # 重みの初期化\n",
    "for i in range(100):\n",
    "    _cost, _ = sess.run([cost, train], feed_dict={x: data_X, t: data_y})\n",
    "    if (i+1)%10==0:\n",
    "        print('iteration:: %d, cost:: %.3f' % (i+1, _cost))\n",
    "\n",
    "# Step6. 予測\n",
    "print('pred_y:', sess.run(y, feed_dict={x: [5]}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. プレースホルダー・変数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfには2種類の変数 (のようなもの) があります. それぞれ以下のように使い分けます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.placeholder`: データ間で値が共有されない変数 (入力の`x`, 正解ラベルの `t` などに使用)\n",
    "- `tf.Variable` : データ間で値が共有される変数 (重みの `W`, `b` など更新されるものに使用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.  `tf.placeholder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを流し込む入り口として使います.\n",
    "- 変数の型 (`tf.int32`, `tf.float32`) を指定する必要があります\n",
    "- 実行時にはデータを`feed_dict`で渡す必要があります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y, feed_dict={x: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. `tf.Variable`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "値がデータ間で共有されるので, まず初期値を与える必要があります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 全ての変数を初期化する場合は`tf.global_variables_initializer()`を使います.\n",
    "- 個別に変数を初期化する場合は`tf.variables_initializer()`を使い, 引数に初期化したい変数をリストで渡します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(0.0, name='w')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(w.eval()) # print(sess.run(w))でも同じです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(0.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.variables_initializer([w]))\n",
    "    print(w.eval()) \n",
    "    print(b.eval()) # 初期化していないので, エラーが出ます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIは`numpy`と非常に似ています. また, `numpy`と同じく要素毎に演算が行われます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "exp_x = tf.exp(x)\n",
    "log_x = tf.log(x)\n",
    "sqrt_x = tf.sqrt(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([exp_x, log_x, sqrt_x], feed_dict={x: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニューラルネットワーク用の関数は`tf.nn`以下にあります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "sigmoid_x = tf.nn.sigmoid(x)\n",
    "tanh_x = tf.nn.tanh(x)\n",
    "relu_x = tf.nn.relu(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([sigmoid_x, tanh_x, relu_x], feed_dict={x: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `np.mean, np.sum`等に対応するものは`tf.reduce_mean, tf.reduce_sum`等になります.\n",
    "- 引数`axis`で指定した軸に沿って演算を行います."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "sum_x = tf.reduce_sum(x, 0)\n",
    "mean_x = tf.reduce_mean(x, 0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([sum_x, mean_x], feed_dict={x: np.arange(10)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 行列・テンソル積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np` の `dot`, `matmul` に対応するものは `tf.matmul` ですが, 少し挙動が違うので注意する必要があります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. 行列積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.matmul` を使用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = tf.ones([2,2])\n",
    "b = tf.ones([2,2])\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ベクトルに対しても `tf.newaxis` などで明示的に行列に変換する必要があります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.ones([2,2])\n",
    "b = tf.ones(2)\n",
    "\n",
    "# c = tf.matmul(a, b) # エラー\n",
    "c = tf.matmul(a, b[:, tf.newaxis])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. テンソル積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```np```と同様に```tf```にも```einsum```があります. 3階以上のテンソルを含む計算はこれを用いるのがわかりやすくベターです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.ones([2,3,4])\n",
    "b = tf.ones([2,3])\n",
    "\n",
    "c = tf.einsum('ijk,ij->k', a, b)\n",
    "sum_c = tf.einsum('ijk,ij->', a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(c.eval())\n",
    "    print(sum_c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. 条件・比較演算子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件演算子は\n",
    "- `tf.cond(condition, if true(関数), if false(関数))`\n",
    "です."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, name='x')\n",
    "y = tf.placeholder(tf.float32, name='y')\n",
    "\n",
    "absl = tf.cond(x > y, lambda: x - y, lambda: y - x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(absl, feed_dict={x: 100, y:  50}))\n",
    "    print(sess.run(absl, feed_dict={x:  50, y: 100}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比較演算子は\n",
    "- `tf.equal`\n",
    "- `tf.greater` (>でも可)\n",
    "- `tf.less` (<でも可)\n",
    "\n",
    "などを使います."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, name='x')\n",
    "y = tf.placeholder(tf.float32, name='y')\n",
    "\n",
    "absl = tf.cond(tf.greater(x, y), lambda: x - y, lambda: y - x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(absl, feed_dict={x: 100, y:  50}))\n",
    "    print(sess.run(absl, feed_dict={x:  50, y: 100}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他の言語のfor文に対応するものは`tf.scan`ですが, これはRNNの回で扱います."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 勾配 (微分) の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.gradients`をつかうことで微分を計算することができます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, name='x')\n",
    "y = x**2\n",
    "\n",
    "grads = tf.gradients(y, x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(grads, feed_dict={x: 1.}))\n",
    "    print(sess.run(grads, feed_dict={x: 2.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二引数(`xs`)に複数の変数を指定すると, それぞれに対する偏微分をリストで返します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = tf.placeholder(tf.float32, name='x1')\n",
    "x2 = tf.placeholder(tf.float32, name='x2')\n",
    "y = 3*x1**2 + 2*x2**4\n",
    "\n",
    "grads = tf.gradients(y, [x1, x2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(grads, feed_dict={x1: 1, x2: 2}))\n",
    "    print(sess.run(grads, feed_dict={x1: 3, x2: 4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 変数 (Variable) の更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assign, assign_add, assign_sub` メソッドにより行います."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(0.0, name='w')\n",
    "\n",
    "add_one = a.assign_add(1.)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        print(sess.run(add_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数の更新をまとめる場合は `tf.group` を使用します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(0.0, name='w')\n",
    "b = tf.Variable(10.0, name='b')\n",
    "\n",
    "add_one = a.assign_add(1.)\n",
    "sub_one = b.assign_sub(1.)\n",
    "\n",
    "updates = [\n",
    "    add_one,\n",
    "    sub_one\n",
    "]\n",
    "\n",
    "train = tf.group(*updates)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        sess.run(train)\n",
    "        print('a:', a.eval(), end=',  ')\n",
    "        print('b:', b.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. TensorBoardによるグラフの表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowで計算グラフの構築方法を扱ってきましたが, ここでは構築した計算グラフの可視化をし, 視覚的に捉えてみましょう.\n",
    "\n",
    "計算グラフを表示するには, tensorboard.py [\\[引用元\\]](http://qiita.com/kegamin/items/887c7dfe8bbb76197741) を読み込む必要があります.\n",
    "\n",
    "tensorboard.pyをimportしたら, `show_graph`関数にグラフを渡すことで可視化できます.\n",
    "\n",
    "可視化結果はインタラクティブな表示になるので, 拡大や移動, 詳細表示等を試してみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([c], feed_dict={a:2, b:3}))\n",
    "\n",
    "tb.show_graph(sess.graph)    # 単純な足し算のグラフの表示 (がしたいが...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. グラフの管理と整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1. デフォルトグラフ\n",
    "\n",
    "グラフが表示されたものの, 大変ごちゃごちゃしており, 足し算以外のグラフも表示されてしまったかと思います.\n",
    "\n",
    "これは, 今回の演習でこれまでに実行された計算グラフがすべて表示されてしまっているためです.\n",
    "\n",
    "TensorFlowでは何も指定しなければ, デフォルトグラフと呼ばれるグラフ上に計算グラフを構築します.\n",
    "\n",
    "一度計算グラフ上に配置されたグラフは, そのグラフを使うか使わないかにかかわらず, 全てリセットされることなく蓄積されていきます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 今までの実行によりデフォルトグラフ上に溜まったオペレーション\n",
    "tf.get_default_graph().get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables() # Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.  `tf.reset_default_graph`関数によるリセット\n",
    "\n",
    "このままでは, 使わないゴミリソースが蓄積してしまう上, TensorBoardで可視化する際にも無関係のグラフまで表示され見づらくなってしまいます.\n",
    "\n",
    "特にJupyter等のインタラクティブな環境では全体で1セッションなので, こうした傾向が顕著であり, 途中でリセット処理を書くことが重要です.\n",
    "\n",
    "対処法としては, 毎回新しくグラフを構築する際に `tf.reset_default_graph()` によりグラフをリセットすることです.\n",
    "\n",
    "こうすることで毎回クリーンな状態でグラフを構築していくことができます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフのリセット\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.global_variables())\n",
    "\n",
    "# 再び足し算のグラフを構築・表示\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c, feed_dict={a:2, b:3}))\n",
    "\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.global_variables())\n",
    "tb.show_graph(tf.Session().graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3. 複雑なグラフの整理\n",
    "\n",
    "グラフを表示したとき, 各ノードの名前は基本的に自動で割り振られます.\n",
    "\n",
    "ただ, これでは少し規模が大きくなるだけですぐにコードとの対応をつけるのが難しくなります.\n",
    "\n",
    "そこで, 定数・変数やプレースホルダーなどには `name`引数を明示的に指定し, ノード名を与えておきましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 線形回帰の例\n",
    "\n",
    "# グラフのリセット\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# プレースホルダーと変数の宣言\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "t = tf.placeholder(tf.float32, name='t')\n",
    "W = tf.Variable(tf.random_uniform([5,3], -1.0, 1.0), name='W')\n",
    "b = tf.Variable(tf.zeros([3]), name='b')\n",
    "\n",
    "# グラフの構築\n",
    "y = tf.add(tf.matmul(x, W), b, name='y')\n",
    "\n",
    "# 誤差関数の定義\n",
    "loss = tf.reduce_mean((y - t)**2, name='loss')\n",
    "\n",
    "tb.show_graph(tf.Session().graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "かなりコードと対応がつき, 見やすくなりました.\n",
    "\n",
    "しかし, 今後より大規模なグラフを扱う際には, 今のままでは頂点が余りにも多くなってしまい, 見ずらくなってしまいます.\n",
    "\n",
    "そこで頂点をまとめることを考えると, これには tf.name_scope 関数を用います.\n",
    "\n",
    "まとめられた頂点は, カーソルをかざすと出てくる右上のプラスマークをクリックすることで展開することができます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# グラフのリセット\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "t = tf.placeholder(tf.float32, name='t')\n",
    "\n",
    "with tf.name_scope('variables'):\n",
    "    W = tf.Variable(tf.random_uniform([5,3], -1.0, 1.0), name='W')\n",
    "    b = tf.Variable(tf.zeros([3]), name='b')\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    y = tf.add(tf.matmul(x, W), b, name='y')\n",
    "\n",
    "with tf.name_scope('training'):\n",
    "    loss = tf.reduce_mean(tf.square(y - t), name='loss')\n",
    "\n",
    "tb.show_graph(tf.Session().graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4. グラフの切り分け\n",
    "\n",
    "デフォルトグラフではなく, 明示的にグラフオブジェクトを作成し, その上にグラフを構築していくことでグラフ環境を他と分けることもできます.\n",
    "\n",
    "これは複数のグラフを構築していきたいときなどに便利です."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフのリセット\n",
    "\n",
    "g0 = tf.get_default_graph() # デフォルトグラフオブジェクトを取得することも可能\n",
    "\n",
    "g1 = tf.Graph() # グラフオブジェクトの作成1\n",
    "\n",
    "a = tf.constant(2, name='a0') # これはdefault graphへの配置になるので注意\n",
    "b = a**a\n",
    "\n",
    "with g1.as_default(): # デフォルトに設定した上で, グラフを構築・操作\n",
    "    a = tf.constant(2, name='a')\n",
    "    b = a**a\n",
    "\n",
    "g2 = tf.Graph() # グラフオブジェクトの作成2\n",
    "\n",
    "with g2.as_default(): # デフォルトに設定し, グラフを構築\n",
    "    a = tf.constant(4, name='a')\n",
    "    x = tf.constant(3, name='x')\n",
    "    y = a**x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb.show_graph(g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=g1) as sess:\n",
    "    print(sess.run(b))\n",
    "tb.show_graph(g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=g2) as sess:\n",
    "    print(sess.run(y))\n",
    "tb.show_graph(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※注意\n",
    "\n",
    "ここで紹介している方法は, グラフの表示のみに対応しています.\n",
    "\n",
    "TensorBoardそのものはグラフ以外にも, 学習中のパラメータの変化など様々な可視化に対応しているのですが,\n",
    "\n",
    "Jupyter上では対応が進んでおらず, TensorBoardをフルに活用できないのです.\n",
    "\n",
    "Jupyterを用いずに手元の環境で行う場合, およそ次の方法でフルのTensorBoardを使用できます. (tensorboard.pyは不要です)\n",
    "\n",
    "1. tf.Session 中で tf.summary.FileWriter 関数によりログの出力を設定\n",
    "\n",
    "2. ターミナルで \"tensorboard --logdir= (ログの出力先) \" を実行\n",
    "\n",
    "3. localhost:6006にブラウザからアクセス\n",
    "\n",
    "より興味が湧いた方はぜひ手元の環境で下の公式情報を参考に他の可視化についても試してください.\n",
    "\n",
    "参考:\n",
    "- https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "- https://www.tensorflow.org/get_started/embedding_viz\n",
    "- https://www.tensorflow.org/get_started/graph_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 自動微分を使ったロジスティック回帰の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットに OR を用いてロジスティック回帰を実装してみましょう. パラメータの勾配の計算には `tf` の自動微分機能 `tf.gradients` を使ってみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 予測確率の計算\n",
    "$$\n",
    "    y = \\sigma({\\bf W}^{\\mathrm{T}}{\\bf x} + {\\bf b})\n",
    "$$\n",
    "- 誤差関数: 交差エントロピー\n",
    "$$\n",
    "    E = -\\sum^N_{i=1} [t_i \\log y_i + (1 - t_i) \\log (1 - y_i) ]\n",
    "$$\n",
    "- 勾配降下法によるパラメータの更新 ($\\epsilon$: 学習率)\n",
    "$$\n",
    "\\begin{align*}\n",
    "    {\\bf W}^{(l)} &\\leftarrow {\\bf W}^{(l)} - \\epsilon \\frac{\\partial E}{\\partial {\\bf W}^{(l)}}  \\\\\n",
    "    {\\bf b}^{(l)} &\\leftarrow {\\bf b}^{(l)} - \\epsilon \\frac{\\partial E}{\\partial {\\bf b}^{(l)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフのリセット\n",
    "\n",
    "# Step1. プレースホルダーと変数の定義\n",
    "## プレースホルダー\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "t = tf.placeholder(tf.float32, name='t')\n",
    "\n",
    "## 変数\n",
    "W = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=(2, 1)).astype('float32'), name='W')\n",
    "b = tf.Variable(np.zeros(1).astype('float32'), name='b')\n",
    "\n",
    "# Step2. グラフの構築\n",
    "y = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "# Step3. 誤差関数の定義\n",
    "# cost = -tf.reduce_mean(t*tf.log(y) + (1 - t)*tf.log(1 - y))\n",
    "cost = -tf.reduce_mean(t*tf.log(tf.clip_by_value(y, 1e-10, 1.0)) + (1 - t)*tf.log(tf.clip_by_value(1 - y, 1e-10, 1.0))) # tf.log(0)によるnanを防ぐ\n",
    "\n",
    "# Step4. 更新則の設定\n",
    "gW, gb = tf.gradients(cost, [W, b])\n",
    "updates = [\n",
    "    W.assign_add(-0.01*gW),\n",
    "    b.assign_add(-0.01*gb)\n",
    "]\n",
    "train = tf.group(*updates)\n",
    "\n",
    "# OR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [1]])\n",
    "\n",
    "# Step5. 学習\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10000):\n",
    "        _cost, _ = sess.run([cost, train], feed_dict={x: train_X, t: train_y})\n",
    "        if (i+1)%1000==0:\n",
    "            print(_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 自動微分を使った多層パーセプトロン (Multilayer perceptron, MLP) の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットに MNIST を用いて MLP を実装してみましょう. パラメータの勾配の計算には `tf` の自動微分機能 `tf.gradients` を使ってみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 順伝播\n",
    "$$\n",
    "\\begin{align*}\n",
    "    {\\bf u}^{(1)} &= {\\bf W}^{(1)\\mathrm{T}} {\\bf x} + {\\bf b}^{(1)} \\\\\n",
    "    {\\bf z}^{(1)} &= \\sigma({\\bf u}^{(1)}) \\\\\n",
    "    {\\bf u}^{(2)} &= {\\bf W}^{(2)\\mathrm{T}} {\\bf z^{(1)}} + {\\bf b}^{(2)} \\\\\n",
    "    {\\bf y} &= \\mathrm{softmax} ({\\bf u}^{(2)})\n",
    "\\end{align*}\n",
    "$$\n",
    "- 誤差関数: 多クラス交差エントロピー\n",
    "$$\n",
    "    E = -\\sum^N_{i=1} \\sum^K_{k=1} t_{ik} \\log y_{ik}\n",
    "$$\n",
    "- 勾配降下法によるパラメータの更新 ($\\epsilon$: 学習率)\n",
    "$$\n",
    "\\begin{align*}\n",
    "    {\\bf W}^{(l)} &\\leftarrow {\\bf W}^{(l)} - \\epsilon \\frac{\\partial E}{\\partial {\\bf W}^{(l)}}  \\\\\n",
    "    {\\bf b}^{(l)} &\\leftarrow {\\bf b}^{(l)} - \\epsilon \\frac{\\partial E}{\\partial {\\bf b}^{(l)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフのリセット\n",
    "\n",
    "# Step1. プレースホルダーと変数の定義\n",
    "## Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "## 変数\n",
    "W1 = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=(784, 200)).astype('float32'), name='W1')\n",
    "b1 = tf.Variable(np.zeros(200).astype('float32'), name='b1')\n",
    "W2 = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=(200, 10)).astype('float32'), name='W2')\n",
    "b2 = tf.Variable(np.zeros(10).astype('float32'), name='b2')\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "# Step2. グラフの定義\n",
    "u1 = tf.matmul(x, W1) + b1\n",
    "z1 = tf.nn.sigmoid(u1)\n",
    "u2 = tf.matmul(z1, W2) + b2\n",
    "y = tf.nn.softmax(u2)\n",
    "\n",
    "# Step3. 誤差関数の定義\n",
    "# cost = -tf.reduce_mean(tf.reduce_sum(t*tf.log(y)))\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(t*tf.log(tf.clip_by_value(y, 1e-10, 1.0)))) # tf.log(0)によるnanを防ぐ\n",
    "\n",
    "# Step4. 更新則の設定\n",
    "gW1, gb1, gW2, gb2 = tf.gradients(cost, params)\n",
    "updates = [\n",
    "    W1.assign_add(-0.01*gW1),\n",
    "    b1.assign_add(-0.01*gb1),\n",
    "    W2.assign_add(-0.01*gW2),\n",
    "    b2.assign_add(-0.01*gb2)\n",
    "]\n",
    "\n",
    "train = tf.group(*updates)\n",
    "\n",
    "valid = tf.argmax(y, 1)\n",
    "\n",
    "# MNIST\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "mnist_X, mnist_y = mnist.train.images, mnist.train.labels\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(mnist_X, mnist_y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = train_X.shape[0] // batch_size\n",
    "\n",
    "# Step5. 学習\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        train_X, train_y = shuffle(train_X, train_y, random_state=random_state)\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            sess.run(train, feed_dict={x: train_X[start:end], t: train_y[start:end]})\n",
    "        pred_y, valid_cost = sess.run([valid, cost], feed_dict={x: valid_X, t: valid_y})\n",
    "        print('EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' % (epoch + 1, valid_cost, f1_score(np.argmax(valid_y, 1).astype('int32'), pred_y, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 参考資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Tensorflow Documentation](https://www.tensorflow.org/api_docs/)\n",
    "2. [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/mandelbrot)\n",
    "2. [CS 20SI: Tensorflow for Deep Leaning Research](http://web.stanford.edu/class/cs20si/)\n",
    "3. [CS224d: Tensorflow Tutorial](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
