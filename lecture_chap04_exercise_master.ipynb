{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1. ロジスティック回帰の実装と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. シグモイド関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) *(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データセットの設定と重みの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [1]])\n",
    "test_X, test_y = train_X, train_y\n",
    "\n",
    "# weights\n",
    "W = np.random.uniform(low=-0.08, high=0.08, size=(2, 1)).astype('float32')\n",
    "b = np.zeros(1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train関数とtest関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差関数\n",
    "* 負の対数尤度関数 (Negative Loglikelihood Function）\n",
    "* 交差エントロピー誤差関数ともいう\n",
    "\n",
    "$$ E ( {\\bf \\theta} ) =  -\\sum^N_{i=1} \\left[ t_i \\log y ({\\bf x}_i ; {\\bf \\theta}) + (1 - t_i) \\log \\{ 1 - y ({\\bf x}_i ; {\\bf \\theta}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=1.0):\n",
    "    global W, b # to access variables that defined outside of this function.\n",
    "    \n",
    "    # Forward Propagation\n",
    "    y = sigmoid(np.matmul(x, W) + b)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    delta = y-t\n",
    "    \n",
    "    # Update Parameters\n",
    "    dW = np.matmul(x.T, delta)\n",
    "    db = np.matmul(np.ones(len(x)), delta)\n",
    "    W = W - eps*dW\n",
    "    b = b - eps*db\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    # Test Cost\n",
    "    y = sigmoid(np.matmul(x, W) + b)\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch\n",
    "for epoch in range(1000):\n",
    "    # Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "    cost, pred_y = test(test_X, test_y)\n",
    "\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2. 活性化関数とその微分の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. シグモイド関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ソフトマックス関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x/np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax(x)*(1 - softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tanh関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題3. 多層パーセプトロンの実装と学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データセットの設定と重みの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [0]])\n",
    "test_X, test_y = train_X, train_y\n",
    "\n",
    "# Layer1 weights\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(2, 3)).astype('float32')\n",
    "b1 = np.zeros(3).astype('float32')\n",
    "\n",
    "# Layer2 weights\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(3, 1)).astype('float32')\n",
    "b2 = np.zeros(1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.train関数とtest関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差関数\n",
    "* 負の対数尤度関数 (Negative Loglikelihood Function）\n",
    "* 交差エントロピー誤差関数ともいう\n",
    "\n",
    "$$ E ( {\\bf \\theta} ) =  -\\sum^N_{i=1} \\left[ t_i \\log y ({\\bf x}_i ; {\\bf \\theta}) + (1 - t_i) \\log \\{ 1 - y ({\\bf x}_i ; {\\bf \\theta}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x, t, eps=1.0):\n",
    "    global W1, b1, W2, b2 # to access variables that defined outside of this function.\n",
    "    \n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "    \n",
    "    # Back Propagation (Cost Function: Negative Loglikelihood)\n",
    "    y = z2\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    delta_2 = y - t # Layer2 delta\n",
    "    delta_1 = deriv_sigmoid(u1) * np.matmul(delta_2, W2.T) # Layer1 delta\n",
    "\n",
    "    # Update Parameters Layer1\n",
    "    dW1 = np.matmul(x.T, delta_1)\n",
    "    db1 = np.matmul(np.ones(len(x)), delta_1)\n",
    "    W1 = W1 - eps*dW1\n",
    "    b1 = b1 - eps*db1\n",
    "    \n",
    "    # Update Parameters Layer2\n",
    "    dW2 = np.matmul(z1.T, delta_2)\n",
    "    db2 = np.matmul(np.ones(len(z1)), delta_2)\n",
    "    W2 = W2 - eps*dW2\n",
    "    b2 = b2 - eps*db2\n",
    "\n",
    "    return cost\n",
    "\n",
    "def test(x, t):\n",
    "    # Forward Propagation Layer1\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    z1 = sigmoid(u1)\n",
    "    \n",
    "    # Forward Propagation Layer2\n",
    "    u2 = np.matmul(z1, W2) + b2\n",
    "    z2 = sigmoid(u2)\n",
    "    \n",
    "    y = z2\n",
    "    \n",
    "    # Test Cost\n",
    "    cost = np.sum(-t*np.log(y)-(1-t)*np.log(1-y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch\n",
    "for epoch in range(2000):\n",
    "    # Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "    cost, pred_y = test(test_X, test_y)\n",
    "\n",
    "print(pred_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
